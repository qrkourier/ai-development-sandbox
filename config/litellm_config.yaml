# ============================================================
# LiteLLM â€” Model Routing Configuration
# Manager can modify this file to add cloud providers.
# ============================================================

model_list:
  # Local models via Ollama
  - model_name: "code-local"
    litellm_params:
      model: "ollama/qwen2.5-coder:32b"
      api_base: "http://ollama:11434"

  - model_name: "code-local-small"
    litellm_params:
      model: "ollama/qwen2.5-coder:7b"
      api_base: "http://ollama:11434"

  - model_name: "embeddings"
    litellm_params:
      model: "ollama/nomic-embed-text"
      api_base: "http://ollama:11434"

  # Frontier models (cloud)
  - model_name: "code-frontier"
    litellm_params:
      model: "claude-opus-4-6"
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: "code-frontier-fast"
    litellm_params:
      model: "claude-sonnet-4-6"
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Fallback (populated when operator provides keys)
  # - model_name: "code-fallback"
  #   litellm_params:
  #     model: "gpt-4o"
  #     api_key: "os.environ/OPENAI_API_KEY"

router_settings:
  routing_strategy: "simple-shuffle"
  # Enable semantic routing when embeddings are available:
  # routing_strategy: "semantic-similarity"
  # embedding_model: "ollama/nomic-embed-text"

general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
